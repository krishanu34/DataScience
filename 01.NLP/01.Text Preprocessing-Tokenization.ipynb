{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishanu34/DataScience/blob/main/01.NLP/01.Text Preprocessing-Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFbdFVvWjx2l"
      },
      "source": [
        "## 1. Tokenization\n",
        "- **Tokenization** is the process of splitting text into smaller units called **tokens**.\n",
        "- Tokens can be words, subwords, or characters depending on the application.\n",
        "\n",
        "### Examples:\n",
        "- **Word-level tokenization**  \n",
        "Sentence: \"The cat sat on the mat.\"\n",
        "Tokens: [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "\n",
        "\n",
        "\n",
        "- **Character-level tokenization**  \n",
        "Sentence: \"cat\"\n",
        "Tokens: [\"c\", \"a\", \"t\"]\n",
        "\n",
        "\n",
        "\n",
        "- **Subword tokenization** (common in modern NLP models like BERT, GPT)  \n",
        "Word: \"unhappiness\"\n",
        "Tokens: [\"un\", \"happi\", \"ness\"]\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Corpus\n",
        "- A **corpus** is a large collection of texts used for NLP tasks.\n",
        "- It serves as the dataset for training or analysis.\n",
        "- Example in our case:\n",
        "\n",
        "Corpus = [\n",
        "\"The cat sat on the mat.\",\n",
        "\"The dog barked loudly.\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Documents\n",
        "- A **document** is an individual text (sentence, paragraph, or article) inside the corpus.\n",
        "- Example:\n",
        "\n",
        "Document 1: \"The cat sat on the mat.\"\n",
        "\n",
        "Document 2: \"The dog barked loudly.\"\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Words / Tokens in Documents\n",
        "- **Words (tokens)** are the individual terms obtained by applying tokenization to documents.\n",
        "- Usually preprocessed (lowercased, punctuation removed).\n",
        "- Example:\n",
        "\n",
        "Document 1 tokens: [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "\n",
        "Document 2 tokens: [\"the\", \"dog\", \"barked\", \"loudly\"]\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Vocabulary\n",
        "- The **vocabulary** is the set of unique words across the entire corpus.\n",
        "- Example:\n",
        "\n",
        "Vocabulary = {\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"barked\", \"loudly\"}\n",
        "\n",
        "\n",
        "- Vocabulary size = 8\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”‘ Hierarchy Summary\n",
        "\n",
        "- **Corpus (2 documents)**  \n",
        "  - **Document 1** â†’ tokens: [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]  \n",
        "  - **Document 2** â†’ tokens: [\"the\", \"dog\", \"barked\", \"loudly\"]  \n",
        "\n",
        "- **Vocabulary** (unique words across documents):  \n",
        "  {\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"barked\", \"loudly\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fah6BKkKls7C",
        "outputId": "001e647e-4c5f-4f33-c6a8-5585a8db2c40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "MsVUBL6mmKjZ"
      },
      "outputs": [],
      "source": [
        "corpus = \"\"\"\n",
        "Natural language processing is a subfield of linguistics, computer science, and artificial intelligence.\n",
        "It is concerned with the interaction between computers and human (natural) languages!\n",
        "Specifically, it is concerned with programming computers to process and analyze large amounts of natural language data.\n",
        "Challenges in NLP include speech recognition, natural language understanding, and natural language generation.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sg39XEEBmtTj",
        "outputId": "051d137a-0545-4eb1-c8c2-973577294293"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Natural language processing is a subfield of linguistics, computer science, and artificial intelligence.\n",
            "It is concerned with the interaction between computers and human (natural) languages!\n",
            "Specifically, it is concerned with programming computers to process and analyze large amounts of natural language data.\n",
            "Challenges in NLP include speech recognition, natural language understanding, and natural language generation.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3-HGHHEm5Wl"
      },
      "source": [
        "#### Tokenization\n",
        "##### Sentence --> Paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "KOHRMayzmt7n"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izeghoXosTEc"
      },
      "source": [
        "Install this if you get error\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "fCUheHdBm0oH"
      },
      "outputs": [],
      "source": [
        "sentences = sent_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6BQzQ-myPyO",
        "outputId": "8f00b667-7d11-430f-f120-cf1275ededff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Natural language processing is a subfield of linguistics, computer science, and artificial intelligence.\n",
            "It is concerned with the interaction between computers and human (natural) languages!\n",
            "Specifically, it is concerned with programming computers to process and analyze large amounts of natural language data.\n",
            "Challenges in NLP include speech recognition, natural language understanding, and natural language generation.\n"
          ]
        }
      ],
      "source": [
        "for sentece in sentences:\n",
        "    print(sentece)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcqyyAVgxmMP"
      },
      "source": [
        "#### Tokenization\n",
        "##### Paragrapgh --> Words\n",
        "##### Sentence --> Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ubAJuisPnF9x"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0FAbS7bxw9G",
        "outputId": "94340b15-fa7e-44ab-ef83-d31743a8700b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'is',\n",
              " 'a',\n",
              " 'subfield',\n",
              " 'of',\n",
              " 'linguistics',\n",
              " ',',\n",
              " 'computer',\n",
              " 'science',\n",
              " ',',\n",
              " 'and',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " '.',\n",
              " 'It',\n",
              " 'is',\n",
              " 'concerned',\n",
              " 'with',\n",
              " 'the',\n",
              " 'interaction',\n",
              " 'between',\n",
              " 'computers',\n",
              " 'and',\n",
              " 'human',\n",
              " '(',\n",
              " 'natural',\n",
              " ')',\n",
              " 'languages',\n",
              " '!',\n",
              " 'Specifically',\n",
              " ',',\n",
              " 'it',\n",
              " 'is',\n",
              " 'concerned',\n",
              " 'with',\n",
              " 'programming',\n",
              " 'computers',\n",
              " 'to',\n",
              " 'process',\n",
              " 'and',\n",
              " 'analyze',\n",
              " 'large',\n",
              " 'amounts',\n",
              " 'of',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'data',\n",
              " '.',\n",
              " 'Challenges',\n",
              " 'in',\n",
              " 'NLP',\n",
              " 'include',\n",
              " 'speech',\n",
              " 'recognition',\n",
              " ',',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'understanding',\n",
              " ',',\n",
              " 'and',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'generation',\n",
              " '.']"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywwcBlR8xzTW",
        "outputId": "78698c04-81ce-437b-afab-a621029ba0eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Natural', 'language', 'processing', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.']\n",
            "['It', 'is', 'concerned', 'with', 'the', 'interaction', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', '!']\n",
            "['Specifically', ',', 'it', 'is', 'concerned', 'with', 'programming', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.']\n",
            "['Challenges', 'in', 'NLP', 'include', 'speech', 'recognition', ',', 'natural', 'language', 'understanding', ',', 'and', 'natural', 'language', 'generation', '.']\n"
          ]
        }
      ],
      "source": [
        "for sentece in sentences:\n",
        "    print(word_tokenize(sentece))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "REqdfkmTydNP"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import wordpunct_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32YOBEg6yotW",
        "outputId": "86e54de1-402c-480a-d1dc-911b6201bcae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Natural', 'language', 'processing', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.']\n",
            "['It', 'is', 'concerned', 'with', 'the', 'interaction', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', '!']\n",
            "['Specifically', ',', 'it', 'is', 'concerned', 'with', 'programming', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.']\n",
            "['Challenges', 'in', 'NLP', 'include', 'speech', 'recognition', ',', 'natural', 'language', 'understanding', ',', 'and', 'natural', 'language', 'generation', '.']\n"
          ]
        }
      ],
      "source": [
        "for sentece in sentences:\n",
        "    print(wordpunct_tokenize(sentece))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "3idcg5x2yqUG"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "n0HeE0Eiyz-B"
      },
      "outputs": [],
      "source": [
        "tokenizer=TreebankWordTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y621nd-ly2lX",
        "outputId": "59223a63-7031-4498-ff6d-888adcbc6b35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Natural', 'language', 'processing', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.']\n",
            "['It', 'is', 'concerned', 'with', 'the', 'interaction', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', '!']\n",
            "['Specifically', ',', 'it', 'is', 'concerned', 'with', 'programming', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.']\n",
            "['Challenges', 'in', 'NLP', 'include', 'speech', 'recognition', ',', 'natural', 'language', 'understanding', ',', 'and', 'natural', 'language', 'generation', '.']\n"
          ]
        }
      ],
      "source": [
        "for sentece in sentences:\n",
        "    print(tokenizer.tokenize(sentece))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "FE9REjQXy3yX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNYENpuIs5kroF459XVbQ5r",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
